% ./bin/pyspark
Python 3.10.5 (v3.10.5:f377153967, Jun  6 2022, 12:36:10) [Clang 13.0.0 (clang-1300.0.29.30)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/

Using Python version 3.10.5 (v3.10.5:f377153967, Jun  6 2022 12:36:10)
Spark context Web UI available at http://192.168.0.108:4040
Spark context available as 'sc' (master = local[*], app id = local-1665535908059).
SparkSession available as 'spark'.

>>>#------------------------------------------
>>># create (K, V) pairs as a Python collection
>>>#------------------------------------------
>>> tuples = [('A', 7), ('A', 8), ('A', -4),
... ('B', 3), ('B', 9), ('B', -1),
... ('C', 1), ('C', 5)]
>>>
>>> tuples
[('A', 7), ('A', 8), ('A', -4), ('B', 3), ('B', 9), ('B', -1), ('C', 1), ('C', 5)]

>>>#------------------------------------------
>>># create an RDD[(String, Integer)]
>>>#------------------------------------------
>>> rdd = spark.sparkContext.parallelize(tuples)
>>> rdd.collect()
[('A', 7), ('A', 8), ('A', -4), ('B', 3), ('B', 9), ('B', -1), ('C', 1), ('C', 5)]

>>>#------------------------------------------
>>># creat an RDD with only positive values
>>>#------------------------------------------
>>> positives = rdd.filter(lambda x: x[1] > 0)
>>> positives.collect()
[('A', 7), ('A', 8), ('B', 3), ('B', 9), ('C', 1), ('C', 5)]
>>>
>>> grouped = positives.groupByKey()
>>> grouped.collect()
[
 ('B', <pyspark.resultiterable.ResultIterable object at 0x11c10b580>), 
 ('C', <pyspark.resultiterable.ResultIterable object at 0x11c10b640>), 
 ('A', <pyspark.resultiterable.ResultIterable object at 0x11c10b4f0>)
]

>>>#------------------------------------------
>>># debug grouped RDD
>>>#------------------------------------------
>>> grouped.mapValues(lambda values : list(values)).collect()
[
 ('B', [3, 9]), 
 ('C', [1, 5]), 
 ('A', [7, 8])
]

>>>#------------------------------------------
>>># find (sum, avg) per key
>>>#------------------------------------------
>>> sum_and_avg = grouped.mapValues(lambda v: (sum(v), float(sum(v))/len(v)))
>>> sum_and_avg.collect()
[
 ('B', (12, 6.0)), 
 ('C', (6, 3.0)), 
 ('A', (15, 7.5))
]

>>>#------------------------------------------
>>># find sum of values per key using reduceByKey()
>>>#------------------------------------------
>>> rdd.collect()
[('A', 7), ('A', 8), ('A', -4), ('B', 3), ('B', 9), ('B', -1), ('C', 1), ('C', 5)]
>>> sum_per_key = rdd.reduceByKey(lambda x, y: x+y)
>>> sum_per_key.collect()
[
 ('B', 11), 
 ('C', 6), 
 ('A', 11)
]